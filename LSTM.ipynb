{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_TYPE = \"twitter\"\n",
    "EMBEDDING_VECTOR_SIZE = 50 # should match glove file\n",
    "INPUT_LENGTH = 200\n",
    "N_AUTHORS = 25\n",
    "VOCAB_SIZE = 5000\n",
    "df = pd.read_pickle(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_df.pickle\")\n",
    "\n",
    "with open(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_embedding.pickle\",'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up function for building model\n",
    "Here we use a very basic, somewhat arbitrary LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Activation, Dense, Embedding\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "def build_lstm_model(vocabulary_size, embedding_dim, input_length, embedding_matrix, output_size):\n",
    "    ## create model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dim,\n",
    "            input_length=input_length,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "        )\n",
    "    ) \n",
    "\n",
    "    model.add(Dense(100, activation=\"softmax\"))\n",
    "\n",
    "    model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "    model.add(LSTM(output_size, activation=\"softmax\"))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[metrics.categorical_accuracy, \"accuracy\"],\n",
    "    )\n",
    "    \n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert author labels into categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# need to figure out labeling, unique authors\n",
    "string_labels = df[\"author\"].unique()\n",
    "label_dict = {}\n",
    "for i in range(len(string_labels)):\n",
    "    label_dict[string_labels[i]] = i\n",
    "    \n",
    "\n",
    "labels = df[\"author\"].map(label_dict)\n",
    "categorical_labels = to_categorical(labels, num_classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"input\"].values.tolist()\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "Use the scikit-learn wrapper so that we can perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from functools import partial\n",
    "\n",
    "# set up model\n",
    "model_fn = partial(build_lstm_model, VOCAB_SIZE, EMBEDDING_VECTOR_SIZE, INPUT_LENGTH, embedding_matrix, N_AUTHORS)\n",
    "model = KerasClassifier(model_fn, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 68693 samples\n",
      "68693/68693 [==============================] - 84s 1ms/sample - loss: 2.8299 - categorical_accuracy: 0.2223 - accuracy: 0.2223\n",
      "17174/17174 [==============================] - 7s 401us/sample - loss: 2.4946 - categorical_accuracy: 0.2334 - accuracy: 0.2334\n",
      "Train on 68693 samples\n",
      "68693/68693 [==============================] - 83s 1ms/sample - loss: 2.8365 - categorical_accuracy: 0.2455 - accuracy: 0.2455\n",
      "17174/17174 [==============================] - 7s 380us/sample - loss: 2.2781 - categorical_accuracy: 0.4301 - accuracy: 0.4301\n",
      "Train on 68694 samples\n",
      "68694/68694 [==============================] - 89s 1ms/sample - loss: 2.8279 - categorical_accuracy: 0.2483 - accuracy: 0.2483\n",
      "17173/17173 [==============================] - 7s 430us/sample - loss: 2.2454 - categorical_accuracy: 0.4240 - accuracy: 0.4240\n",
      "Train on 68694 samples\n",
      "68694/68694 [==============================] - 87s 1ms/sample - loss: 2.8354 - categorical_accuracy: 0.2548 - accuracy: 0.2548\n",
      "17173/17173 [==============================] - 7s 396us/sample - loss: 2.2974 - categorical_accuracy: 0.4530 - accuracy: 0.4530\n",
      "Train on 68694 samples\n",
      "43776/68694 [==================>...........] - ETA: 31s - loss: 3.0076 - categorical_accuracy: 0.2055 - accuracy: 0.2055"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "data\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, data, categorical_labels, cv=kfold)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 439,  214,  185, ...,   47,  146, 1170],\n",
       "       [ 439,  214,  185, ...,   47,  146, 1170],\n",
       "       [ 439,  214,  185, ...,   47,  146, 1170],\n",
       "       ...,\n",
       "       [  11,  384,  969, ...,   10,  631,  169],\n",
       "       [ 597, 2156, 2350, ...,  670, 1170, 2371],\n",
       "       [2401,   12, 1008, ..., 2425,  988,  179]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silk-road-author-id",
   "language": "python",
   "name": "silk-road-author-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
