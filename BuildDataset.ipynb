{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility notebook for dataframe prep\n",
    "\n",
    "This notebook outlines the process for building a dataset for use with a prepared csv with authors and posts. This csv can be built using the functions in the `parse_file.py` script. The output of this file is a pickled numpy array of GloVe vector embeddings and sequence encodings of posts.\n",
    "\n",
    "The procedure used to prepare the dataframe can be summarized by:\n",
    "- Filter out posts with low # of words\n",
    "- Filter out long outliers\n",
    "- Select top n occuring authors\n",
    "- Create a vocabulary and a mapping of vocab -> GloVe vector\n",
    "- Create embedding matrix using the vocabulary\n",
    "- Encode each post to create new dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_FILE = \"glove.twitter.27B.50d.txt\"\n",
    "GLOVE_TYPE = \"twitter\"\n",
    "EMBEDDING_VECTOR_SIZE = 50 # should match glove file\n",
    "MIN_POST_LENGTH = 250 # maybe fillers not included\n",
    "INPUT_LENGTH = 200\n",
    "N_AUTHORS = 25\n",
    "MAX_NUM_WORDS=5000\n",
    "POST_FILE = \"files/posts.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run initial prep on the dataframe\n",
    "- Get length of each post and filter on post length\n",
    "- Remove extremely long posts (likely copy/pasted articles)\n",
    "- Select n most represented authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_length(x):\n",
    "    try:\n",
    "        return len(x.split())\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "\n",
    "def prepare_df(df, min_post_length, n_authors):\n",
    "    \n",
    "    # filter posts by post length\n",
    "    df[\"length\"] = df.post.apply(lambda s: get_length(s))\n",
    "    length_filtered_df = df[df[\"length\"] > min_post_length]\n",
    "    \n",
    "    # remove outlier lengths\n",
    "    length_filtered_df = length_filtered_df[length_filtered_df.length < length_filtered_df.length.quantile(.95)]\n",
    "    \n",
    "    # author counts\n",
    "    author_counts = length_filtered_df[\"author\"].value_counts()\n",
    "    \n",
    "    # select n highest authors\n",
    "    author_filtered_df = length_filtered_df[length_filtered_df[\"author\"].isin(author_counts[:n_authors].index.tolist())]\n",
    "    \n",
    "    return author_filtered_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up utility functions for extracting embeddings from GloVe file and building the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embeddings(glove_file):\n",
    "    embeddings_dict = {}\n",
    "    \n",
    "    # only keep words we have glove entries for\n",
    "    rebuilt_vocab = []\n",
    "    \n",
    "    with open(glove_file, 'r') as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            embeddings_dict[word] = values[1:]\n",
    "\n",
    "    return embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(tokenizer, glove_dim, embeddings_dict, max_words):\n",
    "    embedding_matrix = np.zeros((max_words, glove_dim))\n",
    "    for word, i in t.word_index.items():\n",
    "        embedding_vector = embeddings_dict.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this is kind of cyclical and dumb because we're originally filtering on the post length; but, I don't want to compile a full vocabulary and then pare it down so I'm using the post length redundantly for both the initial filter and encoding filter. Might change later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe from the original post file\n",
    "df = pd.read_csv(POST_FILE)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run dataframe preparation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_df = prepare_df(df, MIN_POST_LENGTH, N_AUTHORS)\n",
    "prepared_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build embedding dictionary..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = build_embeddings(GLOVE_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare tokenizer and integer encode the docs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# prepare tokenizer\n",
    "t = Tokenizer(num_words=MAX_NUM_WORDS) \n",
    "# max_num_words has to be higher than actual to account for \n",
    "# vocab loss in GloVe representation\n",
    "\n",
    "t.fit_on_texts(prepared_df[\"post\"])\n",
    "\n",
    "# integer encode the documents\n",
    "output_df = prepared_df\n",
    "encoded_docs = t.texts_to_sequences(prepared_df[\"post\"])\n",
    "padded = pad_sequences(encoded_docs, maxlen=INPUT_LENGTH, padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert all sequences to array.... \n",
    "This conversion is necessary for downstream use of the df.values\n",
    "Without the conversion, the result is a nested numpy array. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df[\"input\"] = list(padded)\n",
    "\n",
    "# temporary fix for numpy df entries \n",
    "output_df[\"input\"] = output_df.input.progress_apply(lambda r: r.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = build_embeddings(GLOVE_FILE)\n",
    "embedding_matrix = build_embedding_matrix(t, EMBEDDING_VECTOR_SIZE, embedding_dict, MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the embedding matrix and dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop null values\n",
    "output_df.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df.to_pickle(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the embedding matrix\n",
    "import pickle\n",
    "matrix_filename = f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_embedding.pickle\"\n",
    "with open(matrix_filename,'wb') as f:\n",
    "    pickle.dump(np.array(embedding_matrix), f)\n",
    "\n",
    "output_df.to_pickle(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_df.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the vocab size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(embedding_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silk-road-author-id",
   "language": "python",
   "name": "silk-road-author-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
