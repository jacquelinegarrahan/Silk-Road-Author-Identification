{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM model for identifying Silk Road authors\n",
    "\n",
    "This notebook contains the code used for building and training an RNN model on sequence representations of silk road posts. The configuration definitions must match those used in building the training dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_TYPE = \"twitter\"\n",
    "EMBEDDING_VECTOR_SIZE = 50 # should match glove file\n",
    "INPUT_LENGTH = 200\n",
    "N_AUTHORS = 25\n",
    "VOCAB_SIZE = 5000\n",
    "df = pd.read_pickle(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_df.pickle\")\n",
    "\n",
    "with open(f\"files/data/{GLOVE_TYPE}_{N_AUTHORS}_{EMBEDDING_VECTOR_SIZE}_{INPUT_LENGTH}_embedding.pickle\",'rb') as f:\n",
    "    embedding_matrix = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up function for building model\n",
    "Here we use a very basic, somewhat arbitrarily defined RNN model. This should be expanded to accept hidden layer size, dropout, activation, and optimizer as arguments. In the context of this work as a final project for EECE 5644, however, the cross validation will only be performed on my CPU, which is inherantly limiting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dropout, Activation, Dense, Embedding\n",
    "from tensorflow.keras import metrics\n",
    "\n",
    "def build_rnn_model(vocabulary_size, embedding_dim, input_length, embedding_matrix, output_size):\n",
    "    ## create model\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            vocabulary_size,\n",
    "            embedding_dim,\n",
    "            input_length=input_length,\n",
    "            weights=[embedding_matrix],\n",
    "            trainable=False,\n",
    "        )\n",
    "    ) \n",
    "    \n",
    "\n",
    "    model.add(Dense(100, activation = \"softmax\"))\n",
    "\n",
    "    model.add(Dropout(0.1, noise_shape=None, seed=None))\n",
    "\n",
    "    model.add(SimpleRNN(10, activation = 'softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        optimizer=\"adam\",\n",
    "        metrics=[metrics.categorical_accuracy],\n",
    "    )\n",
    "    \n",
    "    model_glove.summary()\n",
    "    return model_glove"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert author labels into categorical labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# need to figure out labeling, unique authors\n",
    "string_labels = df[\"author\"].unique()\n",
    "label_dict = {}\n",
    "for i in range(len(string_labels)):\n",
    "    label_dict[string_labels[i]] = i\n",
    "labels = df[\"author\"].map(label_dict)\n",
    "categorical_labels = to_categorical(labels, num_classes=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[\"input\"].values.tolist()\n",
    "data = np.array(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model\n",
    "Use the scikit-learn wrapper so that we can perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from functools import partial\n",
    "\n",
    "# set up model\n",
    "model_fn = partial(build_rnn_model, VOCAB_SIZE, EMBEDDING_VECTOR_SIZE, INPUT_LENGTH, embedding_matrix, N_AUTHORS)\n",
    "model = KerasClassifier(model_fn, epochs=1, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up kfolds and run cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "data\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "scores = cross_val_score(model, data, categorical_labels, cv=kfold)\n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "silk-road-author-id",
   "language": "python",
   "name": "silk-road-author-id"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
